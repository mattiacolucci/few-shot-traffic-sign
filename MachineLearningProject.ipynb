{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring the effectiveness Few-shot learning for traffic sign recognition\n",
        "\n",
        "# ABSTRACT\n",
        "Nowadays, self-driving veichles is one of the main application of Artificial Intelligence (AI) which has been most widely used in daily-life scenarios. These veichles are based on many AI models among which there are traffic sign recognition systems. Different proposals have been provided for these systems, most of which rely on the usage of big datasets such as GTSRB (dataset of German traffic signs). Collecting a big amount of data is one of the main problems of Machine Learning models, which need lots of evidences in order to being able to generalize well. The effort in collecting more data than available can be one of the main aspects to consider during the development of these systems. Furthermore, using a big dataset expose to possible Adversarial attacks, from poisoning to evasion ones, specially in case of out-source training and testing. For these reasons, in this work we investigate the application of state-of-art Few-shot learning models on the traffic sign recognition domain. The aim of this work is to define the effectiveness of these approaches, in contrast to non-few-shot ones, to open the possibilities of build more robust and effortless solutions in this field.\n",
        "\n",
        "# NOTE\n",
        "più peso alla motivazione dell'attacco rispetto alla difficoltà nella raccolta di dati. Quindi focalizzarsi sul problema che attacchi di poisoning necessitano di aggiungere esempi poisonati che sono facilmente scopribili in casi di dataset piccoli (come nel few-shot) mediante un controllo umano dei sample. Attenzione: verificare, in caso questo controllo non si possa fare, se l'attacco è avvantaggiato con un dataset più piccolo rispetto ad uno grande. Se cosi fosse, sarebbe un problema.\n",
        "\n",
        "in the related work there are the works we are trying to improve\n",
        "\n",
        "specificare come ho creato il dataset ridotto\n",
        "\n",
        "testare i modelli anche sul dataset ridotto\n",
        "\n",
        "cercare come vengono valutati i modelli di few shot. Se si usa bootstrap o altre tecniche\n",
        "\n",
        "usare più modelli few shot, non solo CLIP e LDC\n",
        "\n",
        "-  N-way-k-shot (it means N classes and K instances per classes)\n",
        "\n",
        "-  Approfondire discorso support set (set non utilizzato nè durante training nè durante test ma solo durante fase di predizione)\n",
        "\n",
        "\n",
        "# RICERCHE ESISTENTI\n",
        "- <b>FUSED-Net: Enhancing Few-Shot Traffic Sign Detection with Unfrozen Parameters, Pseudo-Support Sets, Embedding Normalization, and Domain Adaptation</b>: https://arxiv.org/html/2409.14852v1 SOLO SU DATASET DEL BANGLADESH\n",
        "\n",
        "- <b>Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign Recognition</b>: questo fa un compare tra modelli few shot sul dataset GTSRB\n",
        "\n",
        "- <b>Few-shot traffic sign recognition with clustering inductive bias and random neural network</b>: questo propone un nuovo modello per few-shot su traffic sign recognition\n",
        "\n",
        "- <b>Meta-YOLO: Meta-Learning for Few-Shot Traffic Sign Detection via Decoupling Dependencies</b>: questo fa traffic sign detection e non recognition (usa YOLO) quindi a noi non ci interessa\n",
        "\n",
        "- <b>Research on a Traffic Sign Recognition Method under Small Sample Conditions</b>: nuovo modello proposto\n",
        "\n",
        "- <b>Self-supervised few-shot learning for real-time traffic sign classification</b>: altro modello introdotto\n",
        "\n",
        "# EVALUATION METRICS\n",
        "\n",
        "Possible evaluation metrics used in the papers above are:\n",
        "Ma precision e recall non sono per binary classification? Lo dovresti fare per ogni classe considerando tutte le altre classi come istanze negative?\n",
        "- Average precision (AP): area under the precision-recall curve\n",
        "- Precision\n",
        "- Recall\n",
        "- A sto punto anche F1 Score? Male non fa\n",
        "- Accuracy rate (consigliato al posto di error rate. più utilizzato in letteratura)\n",
        "-\n",
        "\n",
        "# EVALUATION TECHNIQUES FOR FEW SHOTS ALGORITHMS\n",
        "- <b>Bootstrap</b>\n",
        "- <b>Fine-Grained Image Classification</b>: si valutano e testano gruppi di classi alla volta in modo tale da identificare i gruppi più problematici (nel nostro caso il gruppo dei segnali stradali relativi ai limiti di velocità potrebbero essere piu difficili da classificare rispetto a quelli di pericolo). \"Fine-Grained Image Classification is a task in computer vision where the goal is to classify images into subcategories within a larger category. For example, classifying different species of birds or different types of flowers. This task is considered to be fine-grained because it requires the model to distinguish between subtle differences in visual appearance and patterns, making it more challenging than regular image classification tasks.\"(https://paperswithcode.com/task/fine-grained-image-classification)\n",
        "-<b>Generalization error</b>: How much does accuracy drop when we reduce K (e.g., from K=5 to K=1)?*\n",
        "\n",
        "\n",
        "# EVALUATION TECHNIQUES TO COMPARE THE FEW SHOT VS MANY-SHOT LEARNING\n",
        "- <b>T-test</b> Separiamo il dataset di test in più dataset dove ciascun dataset contiene i sample di un determinato gruppo di segnali stradali (quindi un gruppo di classi) (vedere Fine-Grained Image Classification in  EVALUATION TECHNIQUES FOR FEW SHOTS ALGORITHMS). Siccome i dataset sono indipendenti gli uni dagli altri possiamo tranquillamente applicare il t-test classico per vedere se l'accuracy rate dell'algoritmo few shot e l'accuracy rate dell'algoritmo non-few shot sono significativamente diversi.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XwJPMAS_A27U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "qVluDFxmA63f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1X_N9dmATNzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Code used to create the random dataset:</h2>\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "def copy_random_images(p1, p2, num_images_per_folder=5):\n",
        "    \"\"\"\n",
        "    Copies random images from subfolders in p1 to p2, recreating the subfolder structure.\n",
        "\n",
        "    Args:\n",
        "        p1 (str): Source path containing subfolders with images.\n",
        "        p2 (str): Destination path where the subfolder structure and images will be recreated.\n",
        "        num_images_per_folder (int): Number of random images to copy from each subfolder.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(p1):\n",
        "        raise ValueError(f\"Source path '{p1}' does not exist.\")\n",
        "    \n",
        "    if not os.path.exists(p2):\n",
        "        os.makedirs(p2)\n",
        "\n",
        "    # Iterate through subfolders in p1\n",
        "    for subfolder in os.listdir(p1):\n",
        "        subfolder_path = os.path.join(p1, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            # Create the same subfolder in p2\n",
        "            dest_subfolder_path = os.path.join(p2, subfolder)\n",
        "            os.makedirs(dest_subfolder_path, exist_ok=True)\n",
        "\n",
        "            # Get all image files in the current subfolder\n",
        "            image_files = [f for f in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, f))]\n",
        "            \n",
        "            # Select random images\n",
        "            random_images = random.sample(image_files, min(num_images_per_folder, len(image_files)))\n",
        "\n",
        "            # Copy selected images to the destination subfolder\n",
        "            for image in random_images:\n",
        "                src_image_path = os.path.join(subfolder_path, image)\n",
        "                dest_image_path = os.path.join(dest_subfolder_path, image)\n",
        "\n",
        "                # Open, resize, and save the image\n",
        "                with Image.open(src_image_path) as img:\n",
        "                    img = img.convert(\"RGB\")  # Ensure the image is in RGB format\n",
        "                    img = img.resize((32, 32))  # Resize to 32x32\n",
        "                    img.save(dest_image_path)\n",
        "\n",
        "            print(f\"Copied {len(random_images)} images from '{subfolder_path}' to '{dest_subfolder_path}'.\")\n",
        "\n",
        "p1 = \"ortiginal_images_folder\"\n",
        "p2 = \"destination_images_folder\"\n",
        "num_images_per_folder = 30\n",
        "\n",
        "arr=[\"/train\",\"/val\",\"/test\"]\n",
        "\n",
        "for a in arr:\n",
        "  copy_random_images(p1, p2, num_images_per_folder+a)"
      ],
      "metadata": {
        "id": "lew2aE4D5Nyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "BQLHmo5PTSVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bzVHtTVx3K1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mattiacolucci/few-shot-traffic-sign\n",
        "\n",
        "!pip install ftfy\n",
        "!pip install alisuretool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrhnEOzU3GsB",
        "outputId": "2656391c-c464-4127-8cf7-8ddff46ede99"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'few-shot-traffic-sign'...\n",
            "remote: Enumerating objects: 3866, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/3866)\u001b[K\rremote: Counting objects:   1% (39/3866)\u001b[K\rremote: Counting objects:   2% (78/3866)\u001b[K\rremote: Counting objects:   3% (116/3866)\u001b[K\rremote: Counting objects:   4% (155/3866)\u001b[K\rremote: Counting objects:   5% (194/3866)\u001b[K\rremote: Counting objects:   6% (232/3866)\u001b[K\rremote: Counting objects:   7% (271/3866)\u001b[K\rremote: Counting objects:   8% (310/3866)\u001b[K\rremote: Counting objects:   9% (348/3866)\u001b[K\rremote: Counting objects:  10% (387/3866)\u001b[K\rremote: Counting objects:  11% (426/3866)\u001b[K\rremote: Counting objects:  12% (464/3866)\u001b[K\rremote: Counting objects:  13% (503/3866)\u001b[K\rremote: Counting objects:  14% (542/3866)\u001b[K\rremote: Counting objects:  15% (580/3866)\u001b[K\rremote: Counting objects:  16% (619/3866)\u001b[K\rremote: Counting objects:  17% (658/3866)\u001b[K\rremote: Counting objects:  18% (696/3866)\u001b[K\rremote: Counting objects:  19% (735/3866)\u001b[K\rremote: Counting objects:  20% (774/3866)\u001b[K\rremote: Counting objects:  21% (812/3866)\u001b[K\rremote: Counting objects:  22% (851/3866)\u001b[K\rremote: Counting objects:  23% (890/3866)\u001b[K\rremote: Counting objects:  24% (928/3866)\u001b[K\rremote: Counting objects:  25% (967/3866)\u001b[K\rremote: Counting objects:  26% (1006/3866)\u001b[K\rremote: Counting objects:  27% (1044/3866)\u001b[K\rremote: Counting objects:  28% (1083/3866)\u001b[K\rremote: Counting objects:  29% (1122/3866)\u001b[K\rremote: Counting objects:  30% (1160/3866)\u001b[K\rremote: Counting objects:  31% (1199/3866)\u001b[K\rremote: Counting objects:  32% (1238/3866)\u001b[K\rremote: Counting objects:  33% (1276/3866)\u001b[K\rremote: Counting objects:  34% (1315/3866)\u001b[K\rremote: Counting objects:  35% (1354/3866)\u001b[K\rremote: Counting objects:  36% (1392/3866)\u001b[K\rremote: Counting objects:  37% (1431/3866)\u001b[K\rremote: Counting objects:  38% (1470/3866)\u001b[K\rremote: Counting objects:  39% (1508/3866)\u001b[K\rremote: Counting objects:  40% (1547/3866)\u001b[K\rremote: Counting objects:  41% (1586/3866)\u001b[K\rremote: Counting objects:  42% (1624/3866)\u001b[K\rremote: Counting objects:  43% (1663/3866)\u001b[K\rremote: Counting objects:  44% (1702/3866)\u001b[K\rremote: Counting objects:  45% (1740/3866)\u001b[K\rremote: Counting objects:  46% (1779/3866)\u001b[K\rremote: Counting objects:  47% (1818/3866)\u001b[K\rremote: Counting objects:  48% (1856/3866)\u001b[K\rremote: Counting objects:  49% (1895/3866)\u001b[K\rremote: Counting objects:  50% (1933/3866)\u001b[K\rremote: Counting objects:  51% (1972/3866)\u001b[K\rremote: Counting objects:  52% (2011/3866)\u001b[K\rremote: Counting objects:  53% (2049/3866)\u001b[K\rremote: Counting objects:  54% (2088/3866)\u001b[K\rremote: Counting objects:  55% (2127/3866)\u001b[K\rremote: Counting objects:  56% (2165/3866)\u001b[K\rremote: Counting objects:  57% (2204/3866)\u001b[K\rremote: Counting objects:  58% (2243/3866)\u001b[K\rremote: Counting objects:  59% (2281/3866)\u001b[K\rremote: Counting objects:  60% (2320/3866)\u001b[K\rremote: Counting objects:  61% (2359/3866)\u001b[K\rremote: Counting objects:  62% (2397/3866)\u001b[K\rremote: Counting objects:  63% (2436/3866)\u001b[K\rremote: Counting objects:  64% (2475/3866)\u001b[K\rremote: Counting objects:  65% (2513/3866)\u001b[K\rremote: Counting objects:  66% (2552/3866)\u001b[K\rremote: Counting objects:  67% (2591/3866)\u001b[K\rremote: Counting objects:  68% (2629/3866)\u001b[K\rremote: Counting objects:  69% (2668/3866)\u001b[K\rremote: Counting objects:  70% (2707/3866)\u001b[K\rremote: Counting objects:  71% (2745/3866)\u001b[K\rremote: Counting objects:  72% (2784/3866)\u001b[K\rremote: Counting objects:  73% (2823/3866)\u001b[K\rremote: Counting objects:  74% (2861/3866)\u001b[K\rremote: Counting objects:  75% (2900/3866)\u001b[K\rremote: Counting objects:  76% (2939/3866)\u001b[K\rremote: Counting objects:  77% (2977/3866)\u001b[K\rremote: Counting objects:  78% (3016/3866)\u001b[K\rremote: Counting objects:  79% (3055/3866)\u001b[K\rremote: Counting objects:  80% (3093/3866)\u001b[K\rremote: Counting objects:  81% (3132/3866)\u001b[K\rremote: Counting objects:  82% (3171/3866)\u001b[K\rremote: Counting objects:  83% (3209/3866)\u001b[K\rremote: Counting objects:  84% (3248/3866)\u001b[K\rremote: Counting objects:  85% (3287/3866)\u001b[K\rremote: Counting objects:  86% (3325/3866)\u001b[K\rremote: Counting objects:  87% (3364/3866)\u001b[K\rremote: Counting objects:  88% (3403/3866)\u001b[K\rremote: Counting objects:  89% (3441/3866)\u001b[K\rremote: Counting objects:  90% (3480/3866)\u001b[K\rremote: Counting objects:  91% (3519/3866)\u001b[K\rremote: Counting objects:  92% (3557/3866)\u001b[K\rremote: Counting objects:  93% (3596/3866)\u001b[K\rremote: Counting objects:  94% (3635/3866)\u001b[K\rremote: Counting objects:  95% (3673/3866)\u001b[K\rremote: Counting objects:  96% (3712/3866)\u001b[K\rremote: Counting objects:  97% (3751/3866)\u001b[K\rremote: Counting objects:  98% (3789/3866)\u001b[K\rremote: Counting objects:  99% (3828/3866)\u001b[K\rremote: Counting objects: 100% (3866/3866)\u001b[K\rremote: Counting objects: 100% (3866/3866), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3851/3851), done.\u001b[K\n",
            "remote: Total 3866 (delta 24), reused 3845 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3866/3866), 10.29 MiB | 28.86 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: alisuretool in /usr/local/lib/python3.11/dist-packages (0.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "# allows to import py files from this folder\n",
        "sys.path.append(\"./few-shot-traffic-sign/\")"
      ],
      "metadata": {
        "id": "4Gb5yKmbBgYh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ldc_train import *\n",
        "\n",
        "dataset_name=\"traffic_sign\"\n",
        "shots=16\n",
        "save_dir = dataset_name+\"_100_way_\"+str(shots)+\"_shot\"\n",
        "log_txt_path = Tools.new_dir(os.path.join(LOG_ROOT, save_dir, \"results.txt\"))\n",
        "\n",
        "# Create the configuration for the custom dataset\n",
        "config = Config10Dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    seed=2024,\n",
        "    shots=shots,\n",
        "    backbone=\"ViT-B/16\",\n",
        "    lr=0.001,\n",
        "    batch_size=64,\n",
        "    train_epoch=50,\n",
        "    loss_lambda=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0],   #last is regularization lambda\n",
        "    fuse_type=2,\n",
        "    save_dir=save_dir,\n",
        "    #regularization='elastic_net'\n",
        ")\n",
        "\n",
        "# Initialize the Runner with the custom dataset configuration\n",
        "runner = Runner(config=config)\n",
        "\n",
        "# Start the training process\n",
        "Tools.print(\"Starting training with the custom dataset...\")\n",
        "acc_list = runner.train()\n",
        "Tools.print({\"name\": \"traffic-sign\", \"acc\": acc_list, \"detail\": config.get_detail()}, log_txt_path)\n",
        "\n",
        "# Optionally, evaluate the model\n",
        "Tools.print(\"Evaluating the model...\")\n",
        "test_acc_list = runner.test()\n",
        "Tools.print({\"name\": \"traffic-sign\", \"test_acc\": test_acc_list}, log_txt_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "5s1agHQX7gNa",
        "outputId": "cbf48c73-777a-4eec-f5b5-4a4f74a892f3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f5e995826be2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create the configuration for the custom dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m config = Config10Dataset(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/./few-shot-traffic-sign/ldc_train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, seed, shots, backbone, lr, batch_size, train_epoch, loss_lambda, fuse_type, regularization, save_dir)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0m_dataset_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_dataset_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4-YscaR3ZQP"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwJPMAS_A27U"
      },
      "source": [
        "# Evaluating the Effectiveness of Few-Shot Learning in Traffic Sign Recognition Tasks\n",
        "\n",
        "<h2>ABSTRACT</h2>\n",
        "<p style=\"text-align: justify; font-family: Arial; font-size: 15px; line-height: 1.6; font-weight: normal;margin-right: 20px;\">Nowadays, self-driving veichles is one of the main application of Artificial Intelligence (AI) which has been most widely used in daily-life scenarios. These veichles are based on many AI models among which there are traffic sign recognition systems. Different proposals have been provided for these systems, most of which rely on the usage of big datasets such as GTSRB (dataset of German traffic signs). Collecting a big amount of data is one of the main problems of Machine Learning models, which need lots of evidences in order to being able to generalize well. The effort in collecting more data than available can be one of the main aspects to consider during the development of these systems. Furthermore, using a big dataset expose to possible Adversarial attacks, from poisoning to evasion ones, specially in case of out-source training and testing. For these reasons, in this work we investigate the application of state-of-art Few-shot learning models on the traffic sign recognition domain. The aim of this work is to define the effectiveness of these approaches, in contrast to non-few-shot ones, to open the possibilities of build more robust and effortless solutions in this field.</p>\n",
        "\n",
        "# NOTE\n",
        "più peso alla motivazione dell'attacco rispetto alla difficoltà nella raccolta di dati. Quindi focalizzarsi sul problema che attacchi di poisoning necessitano di aggiungere esempi poisonati che sono facilmente scopribili in casi di dataset piccoli (come nel few-shot) mediante un controllo umano dei sample. Attenzione: verificare, in caso questo controllo non si possa fare, se l'attacco è avvantaggiato con un dataset più piccolo rispetto ad uno grande. Se cosi fosse, sarebbe un problema.\n",
        "\n",
        "in the related work there are the works we are trying to improve\n",
        "\n",
        "specificare come ho creato il dataset ridotto\n",
        "\n",
        "testare i modelli anche sul dataset ridotto\n",
        "\n",
        "cercare come vengono valutati i modelli di few shot. Se si usa bootstrap o altre tecniche\n",
        "\n",
        "usare più modelli few shot, non solo CLIP e LDC\n",
        "\n",
        "-  N-way-k-shot (it means N classes and K instances per classes)\n",
        "\n",
        "-  Approfondire discorso support set (set non utilizzato nè durante training nè durante test ma solo durante fase di predizione)\n",
        "\n",
        "\n",
        "# RICERCHE ESISTENTI\n",
        "- <b>FUSED-Net: Enhancing Few-Shot Traffic Sign Detection with Unfrozen Parameters, Pseudo-Support Sets, Embedding Normalization, and Domain Adaptation</b>: https://arxiv.org/html/2409.14852v1 SOLO SU DATASET DEL BANGLADESH\n",
        "\n",
        "- <b>Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign Recognition</b>: questo fa un compare tra modelli few shot sul dataset GTSRB\n",
        "\n",
        "- <b>Few-shot traffic sign recognition with clustering inductive bias and random neural network</b>: questo propone un nuovo modello per few-shot su traffic sign recognition\n",
        "\n",
        "- <b>Meta-YOLO: Meta-Learning for Few-Shot Traffic Sign Detection via Decoupling Dependencies</b>: questo fa traffic sign detection e non recognition (usa YOLO) quindi a noi non ci interessa\n",
        "\n",
        "- <b>Research on a Traffic Sign Recognition Method under Small Sample Conditions</b>: nuovo modello proposto\n",
        "\n",
        "-  <b>Self-supervised few-shot learning for real-time traffic sign classification</b>: altro modello introdotto\n",
        "\n",
        "<h2>No few-shot models used for the comparison</h2>\n",
        "<div style=\"text-align: justify; font-family: Arial; font-size: 15px; line-height: 1.6; font-weight: normal;margin-right: 20px;\">\n",
        "<ul>\n",
        "<li>An effective automatic traffic sign classification and recognition deep convolutional networks (2022) (Quello di Mattia)\n",
        "<a href=\"https://doi.org/10.1007/s11042-022-12531-w\">Link Paper</a>\n",
        "</li>\n",
        "<li>Multi-column deep neural network for traffic sign classification (2012) (Modello MCDNN)\n",
        "<a href=\"https://doi.org/10.1016/j.neunet.2012.02.023\">Link Paper</a>\n",
        "<br><a href=\"https://github.com/sixftninja/german-traffic-sign-detection/tree/master\">Link GitHub</a>\n",
        "</li>\n",
        "<li>A Low-cost and Ultra-lightweight Binary Neural Network for Traffic Signal Recognition (2025) (Modello BNN) (No GitHub)\n",
        "<a href=\"http://dx.doi.org/10.48550/arXiv.2501.07808\">Link Paper</a>\n",
        "</li>\n",
        "<li>Visual Trasformer (No Paper, solo codice)\n",
        "<a href=\"https://huggingface.co/kelvinandreas/vit-traffic-sign-GTSRB\">Link HuggingFace</a>\n",
        "</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<h2>Few-shot models used for the comparison</h2>\n",
        "<div style=\"text-align: justify; font-family: Arial; font-size: 15px; line-height: 1.6; font-weight: normal;margin-right: 20px;\">\n",
        "<ul>\n",
        "<li>(Quello nostro di Computer Vision) Clip encoder (<a href=\"https://doi.org/10.48550/arXiv.2103.00020\">Link Paper</a>) + LCD to calculate the noise (<a href=\"https://doi.org/10.48550/arXiv.2504.12104\">Link Paper</a>)\n",
        "</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# EVALUATION METRICS\n",
        "\n",
        "Possible evaluation metrics used in the papers above are:\n",
        "Ma precision e recall non sono per binary classification? Lo dovresti fare per ogni classe considerando tutte le altre classi come istanze negative?\n",
        "- Average precision (AP): area under the precision-recall curve\n",
        "- Precision\n",
        "- Recall\n",
        "- A sto punto anche F1 Score? Male non fa\n",
        "- Accuracy rate (consigliato al posto di error rate. più utilizzato in letteratura)\n",
        "-\n",
        "\n",
        "# EVALUATION TECHNIQUES FOR FEW SHOTS ALGORITHMS\n",
        "- <b>Bootstrap</b>\n",
        "- <b>Fine-Grained Image Classification</b>: si valutano e testano gruppi di classi alla volta in modo tale da identificare i gruppi più problematici (nel nostro caso il gruppo dei segnali stradali relativi ai limiti di velocità potrebbero essere piu difficili da classificare rispetto a quelli di pericolo). \"Fine-Grained Image Classification is a task in computer vision where the goal is to classify images into subcategories within a larger category. For example, classifying different species of birds or different types of flowers. This task is considered to be fine-grained because it requires the model to distinguish between subtle differences in visual appearance and patterns, making it more challenging than regular image classification tasks.\"(https://paperswithcode.com/task/fine-grained-image-classification)\n",
        "-<b>Generalization error</b>: How much does accuracy drop when we reduce K (e.g., from K=5 to K=1)?*\n",
        "- <b>Paper on few-shot evaluation techniques</b> https://arxiv.org/pdf/2307.02732\n",
        "\n",
        "# EVALUATION TECHNIQUES TO COMPARE THE FEW SHOT VS MANY-SHOT LEARNING\n",
        "- <b>Paired T-test</b>: Usiamo J dataset differenti per il traffic sign recgnition task (uno è quello tedesco, dobbiamo trovarne altri) e testiamo per ciascuno di essi il modello few-shot ed uno no-few shot. Per ottenere l'error rate per ciascun dataset possiamore usare l'holdout o cross-validation (meglio ques'ultimo perchè citato nel paper come migliore metodo di valutazione per modelli few-shot).\n",
        "1) Usiamo l'holdout assicurandoci di avere stesso identico test set per entrambi i modelli ma training set diverso (per il modello few-shot ovviamente abbiamo bisogno di meno sample rispetto a quello no few-shot).\n",
        "2) In alternativa all'holdout, possiamo usare una k-fold cross validation in questo modo: il numero e la suddivisione dei fold sono gli stessi per entrambi i modelli, tuttavia, ad ogni iterazione della cross validation, usiamo lo stesso test set per entrambi e, per il modello few-shot, anzichè usare tutte i rimanenti fold per il training set ne prendiamo solo una parte. Dato che il numero di sample per classe nel modello few-shot segue la potenza del 2(Y=2,4,8,16 tipicamente), allora dobbiamo fare in modo che ogni fold abbia Y*N(dove N è il numero delle label e Y il numero dei sample per label richiesto dal modello few-shot) con l'accortezza di averne almeno x=Y/2 che siano strutturati in modo tale da avere SOLTANTO Y istanze per ciascuna delle N label. Problema: cosi abbiamo pochi sample nel test set dato che i fold sono piccoli per venire incontro alle esigenze del modello few-shot (J*N sample in ciascun fold)...Soluzione: anzichè prendere un fold alla volta per il test set ne prendiamo tipo 3 (ovviamente ciò vale per entrambi i modelli). Quindi per il no few-shot prendiamo X fold per il test set e utilizziamo i restanti per il training set, mentre per il few-shot prendiamo sempre X fold per il test set e utilizziamo soltanto quelli che ci servono il training. \n",
        "\n",
        "\n",
        "- <b>Intervalli di confidenza</b> A parte dal t-test, per ogni confronto tra due algoritmi ci calcoliamo per entrambe il corrispondente intervallo di confidenza.\n",
        "- <b>Focus su few-shot</b> A parte dal t-test, testiamo anche il modello few-shot sui diversi gruppi di classi (fine-grained few shot) (no train, solo test) per identificare su quali gruppi il modello few-shot si comporta peggio.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVluDFxmA63f"
      },
      "source": [
        "# Nuova sezione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lew2aE4D5Nyk"
      },
      "source": [
        "<h2>Code used to create the random dataset:</h2>\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "def copy_random_images(p1, p2, num_images_per_folder=5):\n",
        "    \"\"\"\n",
        "    Copies random images from subfolders in p1 to p2, recreating the subfolder structure.\n",
        "\n",
        "    Args:\n",
        "        p1 (str): Source path containing subfolders with images.\n",
        "        p2 (str): Destination path where the subfolder structure and images will be recreated.\n",
        "        num_images_per_folder (int): Number of random images to copy from each subfolder.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(p1):\n",
        "        raise ValueError(f\"Source path '{p1}' does not exist.\")\n",
        "    \n",
        "    if not os.path.exists(p2):\n",
        "        os.makedirs(p2)\n",
        "\n",
        "    # Iterate through subfolders in p1\n",
        "    for subfolder in os.listdir(p1):\n",
        "        subfolder_path = os.path.join(p1, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            # Create the same subfolder in p2\n",
        "            dest_subfolder_path = os.path.join(p2, subfolder)\n",
        "            os.makedirs(dest_subfolder_path, exist_ok=True)\n",
        "\n",
        "            # Get all image files in the current subfolder\n",
        "            image_files = [f for f in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, f))]\n",
        "            \n",
        "            # Select random images\n",
        "            random_images = random.sample(image_files, min(num_images_per_folder, len(image_files)))\n",
        "\n",
        "            # Copy selected images to the destination subfolder\n",
        "            for image in random_images:\n",
        "                src_image_path = os.path.join(subfolder_path, image)\n",
        "                dest_image_path = os.path.join(dest_subfolder_path, image)\n",
        "\n",
        "                # Open, resize, and save the image\n",
        "                with Image.open(src_image_path) as img:\n",
        "                    img = img.convert(\"RGB\")  # Ensure the image is in RGB format\n",
        "                    img = img.resize((32, 32))  # Resize to 32x32\n",
        "                    img.save(dest_image_path)\n",
        "\n",
        "            print(f\"Copied {len(random_images)} images from '{subfolder_path}' to '{dest_subfolder_path}'.\")\n",
        "\n",
        "p1 = \"ortiginal_images_folder\"\n",
        "p2 = \"destination_images_folder\"\n",
        "num_images_per_folder = 30\n",
        "\n",
        "arr=[\"/train\",\"/val\",\"/test\"]\n",
        "\n",
        "for a in arr:\n",
        "  copy_random_images(p1, p2, num_images_per_folder+a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQLHmo5PTSVe"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzVHtTVx3K1m"
      },
      "source": [
        "<h2>Few-shot models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "5s1agHQX7gNa",
        "outputId": "cbf48c73-777a-4eec-f5b5-4a4f74a892f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-22 00:12:08 Preparing ViT-B/16 model.\n",
            "2025-05-22 00:12:09 Getting cached textual weights W ...\n",
            "2025-05-22 00:12:11 Preparing traffic-signs dataset.\n",
            "2025-05-22 00:12:11 Starting training with the custom dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 0:   0%|          | 0/11 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "from ldc_train import *\n",
        "\n",
        "dataset_name=\"traffic-signs\"\n",
        "shots=16\n",
        "save_dir = dataset_name+\"_100_way_\"+str(shots)+\"_shot\"\n",
        "log_txt_path = Tools.new_dir(os.path.join(LOG_ROOT, save_dir, \"results.txt\"))\n",
        "\n",
        "# Create the configuration for the custom dataset\n",
        "config = Config10Dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    seed=2024,\n",
        "    shots=shots,\n",
        "    backbone=\"ViT-B/16\",\n",
        "    lr=0.001,\n",
        "    batch_size=64,\n",
        "    train_epoch=50,\n",
        "    loss_lambda=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0],   #last is regularization lambda\n",
        "    fuse_type=2,\n",
        "    save_dir=save_dir,\n",
        "    #regularization='elastic_net'\n",
        ")\n",
        "\n",
        "# Initialize the Runner with the custom dataset configuration\n",
        "runner = Runner(config=config)\n",
        "\n",
        "# Start the training process\n",
        "Tools.print(\"Starting training with the custom dataset...\")\n",
        "acc_list = runner.train()\n",
        "Tools.print({\"name\": \"traffic-sign\", \"acc\": acc_list, \"detail\": config.get_detail()}, log_txt_path)\n",
        "\n",
        "# Optionally, evaluate the model\n",
        "Tools.print(\"Evaluating the model...\")\n",
        "test_acc_list = runner.test()\n",
        "Tools.print({\"name\": \"traffic-sign\", \"test_acc\": test_acc_list}, log_txt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>NON-Few-shot models</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-YscaR3ZQP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "\n",
        "num_classes = 43\n",
        "\n",
        "#############################################\n",
        "# CNN\n",
        "#############################################\n",
        "\n",
        "cnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (5,5), activation=\"relu\",input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\",input_shape=(28, 28, 32)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\",input_shape=(13, 13, 32)),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\",input_shape=(13, 13, 64)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),  #converts tensor into 1d vector\n",
        "    tf.keras.layers.Dense(units=43,activation=tf.keras.activations.softmax)\n",
        "])\n",
        "\n",
        "# Load the weights\n",
        "cnn_model.load_weights(\"./model/CNN/cnn_model_weights.h5\")\n",
        "\n",
        "print(\"#########################\\nCNN\\n#########################\\n\",cnn_model.summary())\n",
        "\n",
        "#############################################\n",
        "# MDCNN\n",
        "#############################################\n",
        "\n",
        "class MCDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MCDNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 150, kernel_size=7)\n",
        "        self.bn1 = nn.BatchNorm2d(150)\n",
        "        self.conv2 = nn.Conv2d(150, 200, kernel_size=4)\n",
        "        self.bn2 = nn.BatchNorm2d(200)\n",
        "        self.conv3 = nn.Conv2d(200, 300, kernel_size=4)\n",
        "        self.bn3 = nn.BatchNorm2d(300)\n",
        "        self.fc1 = nn.Linear(300 * 3 * 3, 350)\n",
        "        self.bn4 = nn.BatchNorm1d(350)\n",
        "        self.fc2 = nn.Linear(350, num_classes)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv_drop = nn.Dropout2d(p=0.2)\n",
        "        self.fc_drop = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.conv_drop(self.pool(x))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.conv_drop(self.pool(x))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.conv_drop(self.pool(x))\n",
        "        x = x.view(-1, 300 * 3 * 3)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.fc2(self.fc_drop(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "mcdnn_model = MCDNN()\n",
        "\n",
        "mcdnn_model.load_state_dict(torch.load(\"./model/MCDNN/mcdnn_model_weights.pth\"))\n",
        "\n",
        "print(\"\\n\\n#########################\\nMDCNN\\n#########################\\n\",mcdnn_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
